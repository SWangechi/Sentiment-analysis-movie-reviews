# -*- coding: utf-8 -*-
"""SentimentAnalysisOnMovieReviews (1) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15gEIIS4w33B0Azdi4Ierq14jvCKjVoX2

# Sentiment Analysis on Movie Reviews
This dataset contains two columns: review and sentiment. The review column includes text data **(movie reviews)**, and the sentiment column labels each review as either `"positive"` or `"negative."` The main aim is to develop a model that can predict positive or negative reviews of different movie sentiments.

## Import Libraries
Import all necessary libraries to be used in the analysis process.
"""

import pandas as pd
import numpy as np
import re
import nltk
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import ne_chunk, pos_tag
from bs4 import BeautifulSoup
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix
from sklearn import preprocessing

# Download necessary NLTK resources

nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')

"""### Load the dataset
Load the dataset and display the first few rows of the datatset.
"""

movie_df = pd.read_csv('C:/Users/marke/Downloads/IMDB Dataset.csv')

movie_df.head(30)

"""## Quick overview of the data"""

print(movie_df.isnull().sum())

movie_df.dtypes

"""# Visualize the Distribution of Reviews (Positive vs. Negative)
This step visualizes how many reviews are positive and how many are negative.

The reviews are equal which means our dataset will not be biased.
"""

movie_df['sentiment'].value_counts().plot(kind='bar', color=['blue', 'orange'])
plt.title('Distribution of Reviews (Positive vs. Negative)')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

"""# Step 1: Text Preprocessing

### Remove named entities (NER)
Named entities(e.g., names of people, places) which may not contribute to meaningful classification are removed.
"""

# Function to remove named entities
def remove_named_entities(text):
    # Tokenization and Part-of-Speech tagging
    tokens = word_tokenize(text)
    pos_tags = pos_tag(tokens)

    # Named Entity Recognition
    named_entities = ne_chunk(pos_tags)

    # Remove named entities by skipping chunks that represent entities
    cleaned_tokens = []
    for chunk in named_entities:
        if isinstance(chunk, nltk.Tree):  # If it's a named entity
            entity_label = chunk.label()
            if entity_label not in ['PERSON', 'ORGANIZATION', 'GPE']:  # Skip these entity types
                # If it's not a target entity, add the tokens back
                cleaned_tokens.extend([token for token, pos in chunk.leaves()])
        else:
            # If it's not a named entity, add the token back
            token, pos = chunk
            cleaned_tokens.append(token)

    # Join tokens back into a string
    return ' '.join(cleaned_tokens)

# Apply the function to the DataFrame
movie_df['cleaned_review'] = movie_df['review'].apply(remove_named_entities)

# Display the results
movie_df.head(5)

"""### Convert all text to lowercase

This ensures that all text is uniform (e.g., "Great" and "great" are treated the same).
"""

movie_df['cleaned_review'] = movie_df['cleaned_review'].str.lower()

movie_df.head(5)

"""## Remove HTML tags from text in the dataset

This ensures that the dataset does not have any HTML Markup that might interfere with analysis
"""

from bs4 import BeautifulSoup
import re

def remove_html(text):
    if isinstance(text, str):
        try:
            # Remove HTML using BeautifulSoup
            clean_text = BeautifulSoup(text, "html.parser").get_text()

            # Remove standalone "br" from the text
            clean_text = re.sub(r'\bbr\b', '', clean_text)

            # Remove any unwanted characters (like remaining HTML artifacts) using regex
            clean_text = re.sub(r'\s+', ' ', clean_text)  # Remove excessive whitespace
            clean_text = re.sub(r'[^\w\s]', '', clean_text)  # Remove punctuation except word characters and spaces

            # Remove extra spaces left after removing "br"
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()

            return clean_text.lower()  # Convert to lowercase
        except Exception as e:
            print(f"Error processing text: {e}")
            return text
    return text

# Apply the function to the 'cleaned_review' column
movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_html)

# Display the first few rows to verify changes
movie_df.head(5)

"""### Remove URLs, measurements, numbers, filler words, special characters, and punctuation

### Remove URLs
URLs are not useful for sentiment analysis thus removing them ensures the dataset is cleaner
"""

def remove_urls(text):
    return re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_urls)
movie_df.head(5)

"""### Remove both numerical and written numbers

This ensures that the numbers do not interfere with the text-based analysis.
"""

def remove_numbers(text):
    text = re.sub(r'\b\d+\b', '', text)  # Remove numerical digits
    text = re.sub(r'\b(one|two|three|four|five|six|seven|eight|nine|ten)\b', '', text)  # Remove written numbers
    return text

movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_numbers)
movie_df.head(5)

"""### Remove measurements
Irrelevant units of measurements that do not contribute to sentiment classification are removed.
"""

def remove_measurements(text):
    # Regular expression to match measurements (e.g., numbers followed by units)
    # Example units: kg, meters, cm, lb, g, etc.
    pattern = r'\b\d+\.?\d*\s?(kg|cm|meter|meters|mm|g|lb|oz|km|miles|inch|inches|feet|ft)\b'

    # Remove all measurements from the text
    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)

    return cleaned_text

movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_measurements)
movie_df.head(5)

"""### **Remove Emojis**
This ensures that reviews do not contain emojis or smileys that could interfere with accurate text processing and analysis.
"""

# !pip install emoji

import emoji

def remove_emojis(text):
    return emoji.replace_emoji(text, replace='')

movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_emojis)
movie_df.head(5)

"""### **Remove Filler Words**

Filler words like "um", "you know", "actually", etc., do not contribute meaningful content to the text. They serve more as conversation fillers or verbal pauses in speech, and when included in text data, they dilute the clarity and focus of the content.
"""

# Define a list of filler words to remove
filler_words = [
    "uh", "um", "er", "probably", "you know", "I mean", "well", "so", "like", "basically",
    "actually", "seriously", "really", "just", "sort of", "kind of", "whatever",
    "honestly", "literally", "right", "yeah", "okay", "alright"
]

# Create a regex pattern from the list of filler words
filler_pattern = r'\b(?:' + '|'.join(filler_words) + r')\b'

# Define a function to remove filler words
def remove_filler_words(text):
    # Substitute filler words with an empty string
    cleaned_text = re.sub(filler_pattern, '', text, flags=re.IGNORECASE)
    # Remove extra spaces left after removing filler words
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    return cleaned_text

# Apply the function to the text data
movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_filler_words)

# Show the original and cleaned reviews
movie_df.head(5)

"""## Remove special characters and punctuation

Special characters and punctuation can vary in usage between different reviews, even if the content is similar. By removing punctuation, the text becomes more uniform and easier to analyze.

### Remove punctuation
"""

import string

def remove_special_chars(text):
    # Remove all punctuation and special characters except letters and numbers
    return re.sub(r'[{}]'.format(re.escape(string.punctuation)), '', text)

# Apply the function to the review column
movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_special_chars)
movie_df.head(5)

"""### Remove special characters"""

import re

def remove_special_chars(text):
    # Remove all characters that are not letters, numbers, or spaces
    return re.sub(r'[^a-zA-Z0-9\s]', '', text)

# Apply the function to the review column
movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_special_chars)
movie_df.head(5)

"""### Tokenization and Removal of Stop Words

This aims to split each review into individual words or tokens, which serve as the foundation for further text analysis.Removing the stopwords such as "the" are removed wwhich are not helpful for classification.
"""

def remove_stop_words(text):
    stop_words = set(stopwords.words('english'))
    additional_stop_words = [
    'movie', 'film', 'one', 'like', 'story', 'character', 'characters', 'just',
    'time', 'really', 'get', 'good', 'much', 'even', 'make', 'way', 'well',
    'first', 'see', 'two', 'also', 'would', 'could', 'many', 'people'
]
    stop_words = stop_words.union(set(additional_stop_words))
    clean_text = ' '.join([word for word in text.split() if word.lower() not in stop_words])
    return clean_text
movie_df['cleaned_review'] = movie_df['cleaned_review'].apply(remove_stop_words)

def tokenize_text(text):
    return word_tokenize(text)
movie_df['tokens'] = movie_df['cleaned_review'].apply(tokenize_text)
movie_df.head()

"""### Add Custom Stopwords"""

# Define custom stopwords for positive and negative reviews
positive_stopwords = ["amazing", "great", "excellent", "best", "wonderful", "awesome", "fantastic", "brilliant"]
negative_stopwords = ["terrible", "horrible", "awful", "worst", "bad", "boring", "disappointing", "poor"]

import nltk
from nltk.corpus import stopwords

# Download stopwords from NLTK if necessary
nltk.download('stopwords')

# Get NLTK's standard English stopwords
nltk_stopwords = set(stopwords.words('english'))

# Combine NLTK stopwords with custom stopwords for good and bad reviews
positive_all_stopwords = nltk_stopwords.union(positive_stopwords)
negative_all_stopwords = nltk_stopwords.union(negative_stopwords)

# Define a function to remove links
def remove_links(text):
    return re.sub(r'http\S+|www\S+|https\S+', '', text)

# Define a function to remove stopwords based on sentiment
def remove_sentiment_stopwords(text, sentiment):
    words = text.split()
    if sentiment == 'positive':
        cleaned_text = ' '.join([word for word in words if word.lower() not in positive_all_stopwords])
    elif sentiment == 'negative':
        cleaned_text = ' '.join([word for word in words if word.lower() not in negative_all_stopwords])
    else:
        cleaned_text = text  # In case there is no sentiment label
    return cleaned_text


# Apply the cleaning functions (based on sentiment)
movie_df['review'] = movie_df.apply(lambda row: remove_sentiment_stopwords(remove_links(row['review']), row['sentiment']), axis=1)

# Show the cleaned reviews
movie_df[['review', 'cleaned_review', 'sentiment']].head()

"""### Stemming

This ensures that words are reduced to their basic form making it easier to group similar words with the same meaning
"""

from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

movie_df['tokens'] = movie_df['tokens'].apply(lambda x: [stemmer.stem(word) for word in x])

# Display the cleaned tokens
movie_df[['review', 'tokens']].head()

movie_df.head(30)

"""<!-- #### Remove numbers -->

## Vectorization

### Convert Text into Numerical Format(TF-IDF vectorizer)
The conversion of text to numerical matrix to be used in the ML algorithms.
"""

# Create TF-IDF vectorizer
vectorizer = TfidfVectorizer(stop_words='english')

# Fit and transform the reviews into a sparse matrix
tfidf_matrix = vectorizer.fit_transform(movie_df['cleaned_review'])

# Get feature names (words in the vocabulary)
feature_names = vectorizer.get_feature_names_out()

# Example: Get the TF-IDF scores for the first review
first_review_scores = tfidf_matrix[0]

# Convert the sparse matrix for the first review to a dictionary
# Using first_review_scores.toarray() would cause MemoryError, so we avoid that
importance_scores = {feature_names[i]: first_review_scores[0, i] for i in first_review_scores.nonzero()[1]}

# Print the TF-IDF scores for the first review
print(importance_scores)

"""### Bag of Words(BoW)

The Bag of Words model is a way of representing text data where each unique word in the corpus is represented as a feature. The model doesn't consider grammar or word order, just the frequency of words in the text.

**Aim**: To convert the movie reviews into a numerical format by counting the frequency of words using the Bag of Words model.
"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
# Initialize CountVectorizer (BoW)
vectorizer = CountVectorizer(stop_words='english')

# Fit and transform the reviews to a BoW matrix (X)
X = vectorizer.fit_transform(movie_df['cleaned_review'])

# Define the target variable (y)
y = movie_df['sentiment']

"""### Word cloud
A Word Cloud visually represents the most frequent words in the text, where the size of each word indicates its frequency or importance.

**Aim**: To visualize the most frequent words in the movie reviews using a word cloud.

* CountVectorizer(): This function converts
the text into a bag of words model by counting the occurrences of each word.

* fit_transform(): This method is applied to fit the vectorizer on the text and transform it into a sparse matrix where each row corresponds to a document, and each column corresponds to a word.

* get_feature_names_out(): This retrieves the vocabulary (list of words) created by the vectorizer.
"""

# pip install wordcloud matplotlib nltk pandas
# Import required libraries
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Join all the cleaned reviews into one large text
all_text = ' '.join(movie_df['cleaned_review'].tolist())

# Create the word cloud object
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)

# Plot the word cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')  # No axis to show
plt.show()

# Generate a word cloud for positive reviews
positive_reviews = ' '.join(movie_df[movie_df['sentiment'] == 'positive']['cleaned_review'].tolist())
positive_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)

# Plot the positive word cloud
plt.figure(figsize=(10, 5))
plt.imshow(positive_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Generate a word cloud for negative reviews
negative_reviews = ' '.join(movie_df[movie_df['sentiment'] == 'negative']['cleaned_review'].tolist())
negative_wordcloud = WordCloud(width=800, height=400, background_color='white').generate(negative_reviews)

# Plot the negative word cloud
plt.figure(figsize=(10, 5))
plt.imshow(negative_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""### Convert sentiment to numerical values (binary: 1 for positive, 0 for negative)"""

y = pd.get_dummies(movie_df['sentiment'], drop_first=True).values.ravel()  # Positive = 1, Negative = 0

"""## Training

### Train-test split
"""

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Show the sizes of the training and testing sets
print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

"""### Naive Bayes
It is fast and efficient model for sentiment analysis, leveraging its strengths in handling high-dimensional text data and providing probabilistic outputs for classification.
"""

nb_model = MultinomialNB()
nb_model.fit(X_train, y_train)
y_pred_nb = nb_model.predict(X_test)
print(f'Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb)}')

"""### Logistic Regression
Provides a baseline for classification tasks, especially when there is a linear relationship between features and the target.
"""

log_reg = LogisticRegression(max_iter=1000)
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)
print(f'Logistic Regression Accuracy: {accuracy_score(y_test, y_pred_log)}')

"""### Random Forest
Handles complex, non-linear relationships and provide a more robust model by averaging multiple decision trees, reducing overfitting.
"""

rf_model = RandomForestClassifier()
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)
rf_accuracy = accuracy_score(y_test, y_pred_rf)
print(f'Random Forest Accuracy: {rf_accuracy:.4f}')

"""### Decision Tree
Creates an interpretable model that shows the importance of features and provides clear decision rules, but with a risk of overfitting on small datasets.
"""

tree_model = DecisionTreeClassifier()
tree_model.fit(X_train, y_train)
y_pred_tree = tree_model.predict(X_test)
tree_accuracy = accuracy_score(y_test, y_pred_tree)
print(f'Decision Tree Accuracy: {tree_accuracy:.4f}')

"""### Confusion matrix for Naive Bayes

* True Positives (1 predicted as 1): 4,297
* True Negatives (0 predicted as 0): 4,279
* False Positives (0 predicted as 1): 682
* False Negatives (1 predicted as 0): 742

 - Naive Bayes has a relatively high number of false negatives compared to the other models, meaning it misclassifies many positive instances as negative

"""

cm_nb = confusion_matrix(y_test, y_pred_nb)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_nb, annot=True, fmt="d", cmap="Blues")
plt.title('Naive Bayes Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

"""### Confusion matrix for Logistic Regression

* True Positives: 4,425
* True Negatives: 4,319
* False Positives: 642
* False Negatives: 614

Logistic Regression has the fewest false negatives, meaning it performs slightly better in correctly identifying positive instances. The overall balance of correct predictions for both classes is better than Naive Bayes.
"""

cm_log = confusion_matrix(y_test, y_pred_log)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title('Logistic Regression Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

"""### Confusion matrix for Random Forest

* True Positives: 4,333
* True Negatives: 4,253
* False Positives: 708
* False Negatives: 706

Random Forest performs similarly to Logistic Regression but with slightly higher false positives and false negatives. However, it still has a good balance between positive and negative classifications.
"""

cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt="d", cmap="Blues")
plt.title('Random Forest Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

"""### Confusion matrix for Decision Tree

* True Positives (TP): 3,669
* True Negatives (TN): 3,682
* False Positives (FP): 1,279
* False Negatives (FN): 1,370

The Decision Tree classifier shows a fairly balanced performance between true positive and true negative classifications. However, it also has a noticeable number of false positives and false negatives. This could indicate that while the model captures both classes reasonably well, there is some room for improvement, particularly in reducing misclassifications.
"""

cm_tree = confusion_matrix(y_test, y_pred_tree)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_tree, annot=True, fmt="d", cmap="Blues")
plt.title('Decision Tree Confusion Matrix')
plt.ylabel('Actual Label')
plt.xlabel('Predicted Label')
plt.show()

"""### Sentiment Classification Model Results

#### 1. Model Performance
- **Logistic Regression Accuracy**: 87.48%
- **Naive Bayes Accuracy**: 85.76%
- **Random Forest Accuracy**: 86.33%
- **Decision Tree Accuracy**: 73.40%

Logistic Regression achieved the best performance, with an accuracy of 88.27%, followed by Random Forest and Naive Bayes.

| Model              | Accuracy  |
|--------------------|-----------|
| Logistic Regression | 87.48%    |
| Naive Bayes         | 85.76%    |
| Random Forest       | 86.33%    |
| Decision Tree       | 73.40%    |

#### 2. Key Insights
- **Text Preprocessing**: Techniques such as cleaning text (removal of numbers, punctuation, and stop words) significantly improved the quality of the data.
- **TF-IDF Representation**: Helped highlight important words, minimizing the influence of frequently occurring, less meaningful terms.
- **Class Imbalance**: Slight class imbalance may have affected the performance of the Naive Bayes model.

#### 3. Challenges
- **Overfitting**: Certain models, particularly Decision Tree and Random Forest, showed signs of overfitting, which led to decreased generalization on unseen data.

#### 4. Conclusion
Logistic Regression provided the highest accuracy and demonstrated reliability for sentiment classification. Future improvements could involve hyperparameter tuning and feature engineering (such as incorporating bigrams or word embeddings) to boost model performance further.t model performance further.

# Create a Simple UI

Install the Required Libraries: First, install the required libraries for packaging and creating the UI.

#### Install Required Libraries
"""

import joblib
import numpy as np
import ipywidgets as widgets
from IPython.display import display
from sklearn.feature_extraction.text import TfidfVectorizer

"""#### Function to Predict Sentiment
Create a function that will predict the sentiment based on the selected model and the userâ€™s input:
"""

def predict_sentiment(review, model_choice):
    # Transform the input review using the vectorizer
    review_vector = vectorizer.transform([review])

    # Predict based on the selected model
    if model_choice == "Logistic Regression":
        model = log_reg
    elif model_choice == "Naive Bayes":
        model = nb_model
    elif model_choice == "Random Forest":
        model = rf_model
    elif model_choice == "Decision Tree":
        model = tree_model

    # Make the prediction
    prediction = model.predict(review_vector)
    probability = model.predict_proba(review_vector)

    sentiment = "Positive" if prediction[0] == 1 else "Negative"
    confidence = np.max(probability)

    return sentiment, confidence

"""## Set Up the Models and Save Them:

Save the trained models using joblib or pickle to later load them in the UI.
"""

import joblib

# Save models
joblib.dump(log_reg, 'logistic_regression_model.pkl')
joblib.dump(nb_model, 'naive_bayes_model.pkl')
joblib.dump(rf_model, 'random_forest_model.pkl')
joblib.dump(tree_model, 'decision_tree_model.pkl')

joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

"""#### Load the Pre-trained Models and TF-IDF Vectorizer"""

# Load the saved models
log_reg = joblib.load('logistic_regression_model.pkl')
nb_model = joblib.load('naive_bayes_model.pkl')
rf_model = joblib.load('random_forest_model.pkl')
tree_model = joblib.load('decision_tree_model.pkl')

# Load the TF-IDF vectorizer
vectorizer = joblib.load('tfidf_vectorizer.pkl')

# pip install streamlit

"""## Create the Streamlit App:

This has been done on Visual Studio as app.py
"""

